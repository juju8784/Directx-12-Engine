\hypertarget{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent}{}\doxysection{GW\+::S\+Y\+S\+T\+EM\+::G\+Concurrent Class Reference}
\label{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent}\index{GW::SYSTEM::GConcurrent@{GW::SYSTEM::GConcurrent}}
Inheritance diagram for GW\+::S\+Y\+S\+T\+EM\+::G\+Concurrent\+:\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[height=2.000000cm]{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent}
\end{center}
\end{figure}
\doxysubsection*{Public Types}
\begin{DoxyCompactItemize}
\item 
typedef force\+Raw\+::\+Events \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_a69c691a1b3159e2c1e6ae5693e4a55e8}{Events}}
\item 
typedef force\+Raw\+::\+E\+V\+E\+N\+T\+\_\+\+D\+A\+TA \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_aed243f275e9da1a8f546d4401d7b930d}{E\+V\+E\+N\+T\+\_\+\+D\+A\+TA}}
\end{DoxyCompactItemize}
\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
{\footnotesize template$<$typename... Args$>$ }\\\mbox{\hyperlink{namespace_g_w_af46a07bcad99edbe1e92a9fc99078617}{G\+W\+::\+G\+Return}} \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_a0fc12c0f8eceedd0fbb918c021f4d096}{Branch\+Singular}} (Args \&\&... parameter)
\item 
{\footnotesize template$<$typename... Args$>$ }\\\mbox{\hyperlink{namespace_g_w_af46a07bcad99edbe1e92a9fc99078617}{G\+W\+::\+G\+Return}} \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_a34afb5c3b400ab299751348023866e61}{Branch\+Parallel}} (Args \&\&... parameter)
\item 
{\footnotesize template$<$typename... Args$>$ }\\\mbox{\hyperlink{namespace_g_w_af46a07bcad99edbe1e92a9fc99078617}{G\+W\+::\+G\+Return}} \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_abe2a2122f327639b3760ef69dd5beae7}{Converge}} (Args \&\&... parameter)
\item 
\mbox{\hyperlink{namespace_g_w_af46a07bcad99edbe1e92a9fc99078617}{G\+Return}} \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_a36ce59dc3b5fdf78d616976603a7a8fd}{Create}} (bool \+\_\+supress\+Events)
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
The \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent}{G\+Concurrent}} is used by end users and even Gateware itself to launch one-\/time parallel thread work. Even though you can create multiple instances of a \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent}{G\+Concurrent}} they all utilize the same background thread pool. \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent}{G\+Concurrent}} is capable of branching flexible single tasks or even optimized mass parallel array workloads. 

Definition at line 70 of file G\+Concurrent.\+h.



\doxysubsection{Member Typedef Documentation}
\mbox{\Hypertarget{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_a69c691a1b3159e2c1e6ae5693e4a55e8}\label{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_a69c691a1b3159e2c1e6ae5693e4a55e8}} 
\index{GW::SYSTEM::GConcurrent@{GW::SYSTEM::GConcurrent}!Events@{Events}}
\index{Events@{Events}!GW::SYSTEM::GConcurrent@{GW::SYSTEM::GConcurrent}}
\doxysubsubsection{\texorpdfstring{Events}{Events}}
{\footnotesize\ttfamily typedef force\+Raw\+:: \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_a69c691a1b3159e2c1e6ae5693e4a55e8}{Events}} \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_a69c691a1b3159e2c1e6ae5693e4a55e8}{G\+W\+::\+S\+Y\+S\+T\+E\+M\+::\+G\+Concurrent\+::\+Events}}}



Events generated by \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent}{G\+Concurrent}}. 

These messages are used to notify observers when tasks complete. \begin{DoxyVerb}Events::SINGULAR_TASK_COMPLETE      A task started by "BranchSingular" has finished.
Events::PARALLEL_TASK_COMPLETE      A task started by "BranchParallel" has finished.
Events::PARALLEL_SECTION_COMPLETE   A specific sub-section of a parallel task has finished.
\end{DoxyVerb}
 

Definition at line 85 of file G\+Concurrent.\+h.

\mbox{\Hypertarget{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_aed243f275e9da1a8f546d4401d7b930d}\label{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_aed243f275e9da1a8f546d4401d7b930d}} 
\index{GW::SYSTEM::GConcurrent@{GW::SYSTEM::GConcurrent}!EVENT\_DATA@{EVENT\_DATA}}
\index{EVENT\_DATA@{EVENT\_DATA}!GW::SYSTEM::GConcurrent@{GW::SYSTEM::GConcurrent}}
\doxysubsubsection{\texorpdfstring{EVENT\_DATA}{EVENT\_DATA}}
{\footnotesize\ttfamily typedef force\+Raw\+:: \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_aed243f275e9da1a8f546d4401d7b930d}{E\+V\+E\+N\+T\+\_\+\+D\+A\+TA}} \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_aed243f275e9da1a8f546d4401d7b930d}{G\+W\+::\+S\+Y\+S\+T\+E\+M\+::\+G\+Concurrent\+::\+E\+V\+E\+N\+T\+\_\+\+D\+A\+TA}}}



E\+V\+E\+N\+T\+\_\+\+D\+A\+TA provided by \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent}{G\+Concurrent}}. 

All events provide the following relevant information. \begin{DoxyVerb}EVENT_DATA::taskSubmissionIndex     Identifies a task based on increasing submission order. (ALWAYS VALID)
EVENT_DATA::completionRange[2]      Reports what section of a parallel operation was completed [start,end]
                                    NOTE: For SINGULAR_TASK_COMPLETE This is always 0-1.
EVENT_DATA::microsecondsElapsed     How long it took for the reported TASK to complete.(quque time inclusive)
                                    NOTE: PARALLEL_SECTION_COMPLETE does NOT factor in quque wait times.
\end{DoxyVerb}
 

Definition at line 96 of file G\+Concurrent.\+h.



\doxysubsection{Member Function Documentation}
\mbox{\Hypertarget{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_a0fc12c0f8eceedd0fbb918c021f4d096}\label{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_a0fc12c0f8eceedd0fbb918c021f4d096}} 
\index{GW::SYSTEM::GConcurrent@{GW::SYSTEM::GConcurrent}!BranchSingular@{BranchSingular}}
\index{BranchSingular@{BranchSingular}!GW::SYSTEM::GConcurrent@{GW::SYSTEM::GConcurrent}}
\doxysubsubsection{\texorpdfstring{BranchSingular()}{BranchSingular()}}
{\footnotesize\ttfamily template$<$typename... Args$>$ \\
\mbox{\hyperlink{namespace_g_w_af46a07bcad99edbe1e92a9fc99078617}{G\+W\+::\+G\+Return}} G\+W\+::\+S\+Y\+S\+T\+E\+M\+::\+G\+Concurrent\+::\+Branch\+Singular (\begin{DoxyParamCaption}\item[{Args \&\&...}]{parameter }\end{DoxyParamCaption})}



Launch a single-\/thread operation to run concurrently with this thread. 

Adds a job to Gateware\textquotesingle{}s internal threadpool, wait for this operation using \char`\"{}\+Converge\char`\"{} or listen for\+: S\+I\+N\+G\+U\+L\+A\+R\+\_\+\+T\+A\+S\+K\+\_\+\+C\+O\+M\+P\+L\+E\+TE


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em \+\_\+single\+Task} & Any function/lambda that has a void return type.\\
\hline
\end{DoxyParams}

\begin{DoxyRetVals}{Return values}
{\em G\+Return\+::\+I\+N\+V\+A\+L\+I\+D\+\_\+\+A\+R\+G\+U\+M\+E\+NT} & You must provide a non-\/null function to execute. \\
\hline
{\em G\+Return\+::\+S\+U\+C\+C\+E\+SS} & We have successfully submitted the task for processing. \\
\hline
\end{DoxyRetVals}


Definition at line 106 of file G\+Concurrent.\+h.

\mbox{\Hypertarget{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_a34afb5c3b400ab299751348023866e61}\label{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_a34afb5c3b400ab299751348023866e61}} 
\index{GW::SYSTEM::GConcurrent@{GW::SYSTEM::GConcurrent}!BranchParallel@{BranchParallel}}
\index{BranchParallel@{BranchParallel}!GW::SYSTEM::GConcurrent@{GW::SYSTEM::GConcurrent}}
\doxysubsubsection{\texorpdfstring{BranchParallel()}{BranchParallel()}}
{\footnotesize\ttfamily template$<$typename... Args$>$ \\
\mbox{\hyperlink{namespace_g_w_af46a07bcad99edbe1e92a9fc99078617}{G\+W\+::\+G\+Return}} G\+W\+::\+S\+Y\+S\+T\+E\+M\+::\+G\+Concurrent\+::\+Branch\+Parallel (\begin{DoxyParamCaption}\item[{Args \&\&...}]{parameter }\end{DoxyParamCaption})}



Launch a multi-\/thread spanning simulatenous array modification operation to run concurrently with this thread. 

Allows you to process large contigious data sets using a divide \& conquer approach. Based on how many \char`\"{}\+\_\+max\+Sections\char`\"{} are required to traverse \char`\"{}\+\_\+array\+Size\char`\"{} we create that many jobs in the internal thread pool. Then each job will run \char`\"{}\+\_\+parallel\+Task\char`\"{} for each input/output pair in it\textquotesingle{}s designated section.(optional array location is provided) To know when a single section completes listen for\+: P\+A\+R\+A\+L\+L\+E\+L\+\_\+\+S\+E\+C\+T\+I\+O\+N\+\_\+\+C\+O\+M\+P\+L\+E\+TE, There is no cross section ordering garuntee. Global resources accessed by the provided operation are your responsibility to synchronize for thread safety. (recc\+: G\+Thread\+Shared) However the provided arrays are safely accessed due to being divided evenly amongst the internal jobs. (no overlapp) You may wait for this entire operation to complete using \char`\"{}\+Converge\char`\"{} or listen for\+: P\+A\+R\+A\+L\+L\+E\+L\+\_\+\+T\+A\+S\+K\+\_\+\+C\+O\+M\+P\+L\+E\+TE Once you have \char`\"{}\+Converged\char`\"{} or received the appropriate message(s) you are safe to acd amongst processing cores. ~\newline



\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em \+\_\+parallel\+Task} & Pointer to static operation responsible for element processing, called once for each specific element. \\
\hline
\mbox{\texttt{ in}}  & {\em \+\_\+max\+Section} & The maximum \# of elements processed by a thread at once, this has the largest impact on perfomance. N\+O\+TE\+: As rule of thumb this value should be smaller for complex operations and larger for simplistic ones. \\
\hline
\mbox{\texttt{ in}}  & {\em \+\_\+array\+Size} & The size of array or problem domain that is to be divideey wish each routine to have access to. \\
\hline
\mbox{\texttt{ in}}  & {\em \+\_\+user\+Data} & {\ttfamily (optional)} Pointer to some custom user provided data that the input/output data arrays might need. \\
\hline
\mbox{\texttt{ in}}  & {\em \+\_\+in\+Stride} & {\ttfamily (optional)} if 0 then sizeof(\+Input) type used for input array traversal, otherwise this byte amount is used. \\
\hline
\mbox{\texttt{ in}}  & {\em \+\_\+input\+Array} & {\ttfamily (optional)} Array of input data elements that will be fed to the processing system. \\
\hline
\mbox{\texttt{ in}}  & {\em \+\_\+out\+Stride} & {\ttfamily (optional)} if 0 then sizeof(\+Output) type used for output array traversal, otherwise this byte amount is used. \\
\hline
\mbox{\texttt{ out}}  & {\em \+\_\+output\+Array} & {\ttfamily (optional)} Array of output data elements that will be fed to the processing system.\\
\hline
\end{DoxyParams}

\begin{DoxyRetVals}{Return values}
{\em G\+Return\+::\+M\+E\+M\+O\+R\+Y\+\_\+\+C\+O\+R\+R\+U\+P\+T\+I\+ON} & \+\_\+input\+Array \& \+\_\+output\+Array have an overlapping memory space. (Just use \+\_\+output\+Array instead) \\
\hline
{\em G\+Return\+::\+I\+N\+V\+A\+L\+I\+D\+\_\+\+A\+R\+G\+U\+M\+E\+NT} & You must provide a non-\/null function to execute and a non-\/zero array\+Size \& max\+Section. Cannot use the same array for \+\_\+input\+Array and \+\_\+output\+Array. In this situation, just use \+\_\+output\+Array. \\
\hline
{\em G\+Return\+::\+S\+U\+C\+C\+E\+SS} & We have successfully submitted the task for processing. \\
\hline
\end{DoxyRetVals}


Definition at line 133 of file G\+Concurrent.\+h.

\mbox{\Hypertarget{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_abe2a2122f327639b3760ef69dd5beae7}\label{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_abe2a2122f327639b3760ef69dd5beae7}} 
\index{GW::SYSTEM::GConcurrent@{GW::SYSTEM::GConcurrent}!Converge@{Converge}}
\index{Converge@{Converge}!GW::SYSTEM::GConcurrent@{GW::SYSTEM::GConcurrent}}
\doxysubsubsection{\texorpdfstring{Converge()}{Converge()}}
{\footnotesize\ttfamily template$<$typename... Args$>$ \\
\mbox{\hyperlink{namespace_g_w_af46a07bcad99edbe1e92a9fc99078617}{G\+W\+::\+G\+Return}} G\+W\+::\+S\+Y\+S\+T\+E\+M\+::\+G\+Concurrent\+::\+Converge (\begin{DoxyParamCaption}\item[{Args \&\&...}]{parameter }\end{DoxyParamCaption})}



Forces the current thread to wait until any \& all branched processing has completed. 

Polls the \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent}{G\+Concurrent}} until all running tasks have completed. Use this when you need to access resources on the current thread that may still be processing elsewhere. If you want a less invasive method of waiting and responding, consider using the built-\/in messaging system to be notified instead.


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em \+\_\+spin\+Until} & Determine how long the current thread will spinlock waiting for all Tasks to complete. (in N\+A\+N\+O\+S\+E\+C\+O\+N\+DS) Once \char`\"{}\+\_\+spin\+Until\char`\"{} nanoseconds has elapsed the current thread will yield to the OS scheduler. (1 millisecond yield) Upon being re-\/scheduled the thread will try to spinlock again until \char`\"{}\+\_\+spin\+Until\char`\"{} is reached again or completion. N\+O\+TE\+: If your not in a time/performance critical area you should just use 0. Wake times dependent on OS resolution.\\
\hline
\end{DoxyParams}

\begin{DoxyRetVals}{Return values}
{\em G\+Return\+::\+F\+A\+I\+L\+U\+RE} & There have been no Tasks ever or since you last Converged. \\
\hline
{\em G\+Return\+::\+S\+U\+C\+C\+E\+SS} & Tasks were running and we sucessfully waited for their completion. \\
\hline
\end{DoxyRetVals}


Definition at line 148 of file G\+Concurrent.\+h.

\mbox{\Hypertarget{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_a36ce59dc3b5fdf78d616976603a7a8fd}\label{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent_a36ce59dc3b5fdf78d616976603a7a8fd}} 
\index{GW::SYSTEM::GConcurrent@{GW::SYSTEM::GConcurrent}!Create@{Create}}
\index{Create@{Create}!GW::SYSTEM::GConcurrent@{GW::SYSTEM::GConcurrent}}
\doxysubsubsection{\texorpdfstring{Create()}{Create()}}
{\footnotesize\ttfamily \mbox{\hyperlink{namespace_g_w_af46a07bcad99edbe1e92a9fc99078617}{G\+Return}} G\+W\+::\+S\+Y\+S\+T\+E\+M\+::\+G\+Concurrent\+::\+Create (\begin{DoxyParamCaption}\item[{bool}]{\+\_\+supress\+Events }\end{DoxyParamCaption})}



Allocates \& Initializes a \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent}{G\+Concurrent}}. 

Creates a \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent}{G\+Concurrent}} proxy which enables you to launch operations on other threads and synchronize with them. The \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent}{G\+Concurrent}} is a straightforward way to add tasks to Gateware\textquotesingle{}s internal threadpool.


\begin{DoxyParams}[1]{Parameters}
\mbox{\texttt{ in}}  & {\em \+\_\+suppress\+Events} & If you only use \char`\"{}\+Converge\char`\"{} or do not require \mbox{\hyperlink{struct_g_w_1_1_g_event}{G\+Event}} notifications, set to false for better perfomance.\\
\hline
\end{DoxyParams}

\begin{DoxyRetVals}{Return values}
{\em G\+Return\+::\+S\+U\+C\+C\+E\+SS} & \mbox{\hyperlink{class_g_w_1_1_s_y_s_t_e_m_1_1_g_concurrent}{G\+Concurrent}} created successfully, ready to branch work. \\
\hline
\end{DoxyRetVals}
